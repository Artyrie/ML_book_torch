{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "scalar1 = torch.tensor([1.])\n",
    "print(scalar1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "scalar2 = torch.tensor([3.])\n",
    "print(scalar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "add_scalar = scalar1 + scalar2\n",
    "print(add_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(scalar1, scalar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.])\n"
     ]
    }
   ],
   "source": [
    "sub_scalar = scalar1 - scalar2\n",
    "print(sub_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sub(scalar1, scalar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "mul_scalar = scalar1 * scalar2\n",
    "print(mul_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(scalar1, scalar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3333])\n"
     ]
    }
   ],
   "source": [
    "div_scalar = scalar1 / scalar2\n",
    "print(div_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3333])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.div(scalar1, scalar2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "vec1 = torch.tensor([1., 2., 3.])\n",
    "print(vec1)\n",
    "\n",
    "vec2 = torch.tensor([4., 5., 6.])\n",
    "print(vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 7., 9.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3., -3., -3.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sub(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4., 10., 18.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(vec1, vec2)\n",
    "# element-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2500, 0.4000, 0.5000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.div(vec1, vec2)\n",
    "# element-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(vec1, vec2)\n",
    "# 내적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "mat1 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(mat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "mat2 = torch.tensor([[5., 6.], [7., 8.]])\n",
    "print(mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  8.],\n",
       "        [10., 12.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(mat1, mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4., -4.],\n",
       "        [-4., -4.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sub(mat1, mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5., 12.],\n",
       "        [21., 32.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(mat1, mat2)\n",
    "# element-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.3333],\n",
       "        [0.4286, 0.5000]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.div(mat1, mat2)\n",
    "# element-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 22.],\n",
       "        [43., 50.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(mat1, mat2)\n",
    "# 행렬곱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor 다루기\n",
    "\n",
    "Tensor는 2차원 이상의 배열\n",
    "논리는 동일하며, 행렬 곱은 텐서 내 같은 행렬 단위의 인덱스 간에 수행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor([[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]])\n",
    "print(tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 9., 10.],\n",
      "         [11., 12.]],\n",
      "\n",
      "        [[13., 14.],\n",
      "         [15., 16.]]])\n"
     ]
    }
   ],
   "source": [
    "tensor2 = torch.tensor([[[9., 10.], [11., 12.]], [[13., 14.], [15., 16]]])\n",
    "print(tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10., 12.],\n",
       "         [14., 16.]],\n",
       "\n",
       "        [[18., 20.],\n",
       "         [22., 24.]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-8., -8.],\n",
       "         [-8., -8.]],\n",
       "\n",
       "        [[-8., -8.],\n",
       "         [-8., -8.]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sub(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  9.,  20.],\n",
       "         [ 33.,  48.]],\n",
       "\n",
       "        [[ 65.,  84.],\n",
       "         [105., 128.]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1111, 0.2000],\n",
       "         [0.2727, 0.3333]],\n",
       "\n",
       "        [[0.3846, 0.4286],\n",
       "         [0.4667, 0.5000]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.div(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 31.,  34.],\n",
       "         [ 71.,  78.]],\n",
       "\n",
       "        [[155., 166.],\n",
       "         [211., 226.]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(tensor1, tensor2)\n",
    "# 같은 인덱스의 행렬 간 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Autograd\n",
    " \n",
    " Back Propagation은 Autograd로 쉽게 구현이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() :\n",
    "    DEVICE = torch.device('cuda')\n",
    "else : \n",
    "    DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "INPUT_SIZE = 1000\n",
    "HIDDEN_SIZE = 100\n",
    "OUTPUT_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(BATCH_SIZE,\n",
    "               INPUT_SIZE,\n",
    "               device = DEVICE,\n",
    "               dtype = torch.float,\n",
    "               requires_grad = False)\n",
    "\n",
    "y = torch.randn(BATCH_SIZE,\n",
    "               OUTPUT_SIZE,\n",
    "               device = DEVICE,\n",
    "               dtype = torch.float,\n",
    "               requires_grad = False)\n",
    "\n",
    "w1 = torch.randn(INPUT_SIZE,\n",
    "               HIDDEN_SIZE,\n",
    "               device = DEVICE,\n",
    "               dtype = torch.float,\n",
    "               requires_grad = True)\n",
    "\n",
    "w2 = torch.randn(HIDDEN_SIZE,\n",
    "               OUTPUT_SIZE,\n",
    "               device = DEVICE,\n",
    "               dtype = torch.float,\n",
    "               requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  100 \t Loss :  3.433090569160413e-06\n",
      "Iteration :  200 \t Loss :  2.8019744604534935e-06\n",
      "Iteration :  300 \t Loss :  2.4026448954828084e-06\n",
      "Iteration :  400 \t Loss :  2.076666760331136e-06\n",
      "Iteration :  500 \t Loss :  1.8375706076767528e-06\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-6\n",
    "\n",
    "for t in range(1, 501) : \n",
    "    y_pred = x.mm(w1).clamp(min = 0).mm(w2) \n",
    "    #clamp는 비선형 함수. ReLU와 같은 역할로 26p 확인\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    if t % 100 == 0 : \n",
    "        print(\"Iteration : \", t, \"\\t\", \"Loss : \", loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad() : \n",
    "        w1 -= lr * w1.grad\n",
    "        w2 -= lr * w2.grad\n",
    "        \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
